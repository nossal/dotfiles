return {
  "huggingface/llm.nvim",
  enabled = false,
  opts = {
  backend = "ollama",
    model = "codellama",
    url = "http://192.168.1.236:11434"
    -- api_token = "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"
    -- cf Setup
  },
}
